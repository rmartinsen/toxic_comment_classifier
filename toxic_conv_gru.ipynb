{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import BatchNormalization, Conv1D, Dense, Dropout, Embedding, Flatten, GRU, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 25000\n",
    "MAX_TEXT_LEN = 100\n",
    "EMBEDDING_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95851"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(train_df[\"comment_text\"].fillna(\"MISSINGVALUE\").values))\n",
    "\n",
    "word_2_index = tokenizer.word_index\n",
    "index_2_word = {ix: word for word, ix in word_2_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_tokens(df):\n",
    "    comment_text = df[\"comment_text\"].astype(str)\n",
    "    tokens = tokenizer.texts_to_sequences(comment_text)\n",
    "    padded_tokens = pad_sequences(tokens, MAX_TEXT_LEN)\n",
    "    return padded_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[train_df.iloc[row][\"toxic\"], train_df.iloc[row][\"severe_toxic\"],\n",
    "                     train_df.iloc[row][\"obscene\"], train_df.iloc[row][\"threat\"], \n",
    "                     train_df.iloc[row][\"insult\"], train_df.iloc[row][\"identity_hate\"]\n",
    "                     ]for row in range(len(train_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding = Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Dropout(.1))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.1))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(GRU(32))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(6, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext_weights = np.load(\"fasttext/fasttext_weights.npy\")\n",
    "# model.layers[0].set_weights(fasttext_weights)\n",
    "# model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         7500000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 32)          19232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 7,538,822\n",
      "Trainable params: 7,538,182\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95851 samples, validate on 95851 samples\n",
      "Epoch 1/1\n",
      "18496/95851 [====>.........................] - ETA: 4:28 - loss: 0.0457 - acc: 0.9832"
     ]
    }
   ],
   "source": [
    "model.fit(x=np.array(X), y=np.array(y), validation_data=(X, y), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/1\n",
      "86265/86265 [==============================] - 291s 3ms/step - loss: 0.0523 - acc: 0.9811 - val_loss: 0.0531 - val_acc: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x131718ef0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=np.array(X_train), y=np.array(y_train), validation_data=(X_test, y_test), epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0.001, patience=4, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91058 samples, validate on 4793 samples\n",
      "Epoch 1/15\n",
      "91058/91058 [==============================] - 2062s 23ms/step - loss: 0.0559 - acc: 0.9799 - val_loss: 0.0560 - val_acc: 0.9808\n",
      "Epoch 2/15\n",
      "91058/91058 [==============================] - 2059s 23ms/step - loss: 0.0484 - acc: 0.9822 - val_loss: 0.0736 - val_acc: 0.9795\n",
      "Epoch 3/15\n",
      "91058/91058 [==============================] - 2058s 23ms/step - loss: 0.0417 - acc: 0.9843 - val_loss: 0.0643 - val_acc: 0.9798\n",
      "Epoch 4/15\n",
      "91058/91058 [==============================] - 2061s 23ms/step - loss: 0.0366 - acc: 0.9861 - val_loss: 0.0645 - val_acc: 0.9801\n",
      "Epoch 5/15\n",
      "91058/91058 [==============================] - 2057s 23ms/step - loss: 0.0333 - acc: 0.9876 - val_loss: 0.0723 - val_acc: 0.9798\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24707e9b0>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=np.array(X_train), y=np.array(y_train), validation_data=(X_test, y_test), epochs=15,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def bin_log_loss(pred, actual, eps=.000001):\n",
    "    pred = eps if pred==0 else pred\n",
    "    pred = 1 - eps if pred==1 else pred\n",
    "    return actual * log(pred) + (1 - actual) * log(1 - pred)\n",
    "\n",
    "def log_loss_6(preds, actual, eps=.000001):\n",
    "    preds = [pred or eps for pred in preds]\n",
    "    losses = [bin_log_loss(preds[x], actual[x]) for x in range(len(preds))]\n",
    "    return -sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "# [(x, log_loss_6(preds[x], y_test[x])) for x in range(len(preds)) if log_loss_6(preds[x], y_test[x]) > 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05307727477144796"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([log_loss_6(preds[x], y_test[x]) for x in range(len(preds))])/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_models/conv_GRU_20180109\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"test.csv\")\n",
    "submission.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submit = create_padded_tokens(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "submission = submission.reindex(columns=pred_columns)\n",
    "submission[pred_columns] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_2_20180108.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226998, 6)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
